# Copyright (c) 2024 Stepfun AI, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
"""
Extendable Trainer classes for aligning LLMs.
The specific class that should be used should be specified in the config.py

The BasicTrainer contains the core methods (e.g., evaluation, basic training loop, etc.).
The SFTTrainer, RewardTrainer, PPOTrainer, KTOTrainer, PairedPreferenceTrainer all subclass BasicTrainer
and override the get_batch_metrics() and (optionally) forward() methods.

The trainer for each loss should subclass either PairedPreferenceTrainer or BasicTrainer.
"""

import torch
from utils.utils import (
    all_gather_if_needed,
    get_batch_logps,
    remove_cache,
    entropy_from_logits,
    move_batch_on_device,
    delete_dict,
    formatted_dict,
    calculate_mean_variance
)
import torch.nn.functional as F
from trainers.basic_trainer import BasicTrainer
from models.models import AutoModelForCausalLMWithScalarHead
from transformers import AutoTokenizer
import torch.distributed as dist
import json
import dataloaders.dataloader as dataloader
import re
import tqdm
import os
from collections import defaultdict
import wandb
import time
import math

class PairedPreferenceTrainer(BasicTrainer):
    """A trainer for any loss that uses paired preference, like DPO/IPO."""
    def forward(self, model, batch, detach=False):
        """Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.
           Return two tensors of shape (batch size), one of the chosen examples, another of the rejected ones.
        Args:
            model:
                the model to forward on, policy model or reference model
            batch:
                the batch of data to froward on
            detach:
                whether to detach to returned tensor

        Returns:
            chosen_logps: log probabilities of chosen examples (should be batch size / 2 if data was read in correctly)
            rejected_logps: log probabilities of rejected examples (should be batch size / 2 if data was read in correctly)
        """

        concatenated_batch = self.concatenated_inputs(batch)
        all_logits = model(concatenated_batch['concatenated_combined_input_ids'], attention_mask=concatenated_batch['concatenated_combined_attention_mask']).logits
        all_logps, all_ppls = get_batch_logps(all_logits, concatenated_batch['concatenated_labels'], token_level=False, eos_id=self.tokenizer.eos_token_id)
        
        loss_mask = (concatenated_batch['concatenated_labels'] != -100)
        all_logps_avg = all_logps/loss_mask.sum(-1)

        num_of_chosen =  batch['chosen_combined_input_ids'].shape[0]
        chosen_logps = all_logps[:num_of_chosen]
        rejected_logps = all_logps[num_of_chosen:]
        chosen_logps_avg = all_logps_avg[:num_of_chosen]
        rejected_logps_avg = all_logps_avg[num_of_chosen:]

        chosen_logits = all_logits[:num_of_chosen]
        rejected_logits = all_logits[num_of_chosen:]

        chosen_masks = (concatenated_batch['concatenated_labels'][:num_of_chosen] != -100).detach().clone().contiguous().to(self.policy_dtype)
        rejected_masks = (concatenated_batch['concatenated_labels'][num_of_chosen:] != -100).detach().clone().contiguous().to(self.policy_dtype)

        chosen_entroy = entropy_from_logits(chosen_logits, chosen_masks)
        rejected_entropy = entropy_from_logits(rejected_logits, rejected_masks)
        chosen_ppl = all_ppls[:num_of_chosen]
        rejected_ppl = all_ppls[num_of_chosen:]

        if detach:
            chosen_logps_detached, rejected_logps_detached, chosen_logps_avg_detached, rejected_logps_avg_detached = chosen_logps.detach().clone(), rejected_logps.detach().clone(), chosen_logps_avg.detach().clone(), rejected_logps_avg.detach().clone()
            del all_logits, all_logps, all_ppls, chosen_logps, rejected_logps, chosen_logits, rejected_logits, chosen_entroy, rejected_entropy, chosen_ppl, rejected_ppl, chosen_logps_avg, rejected_logps_avg
            remove_cache()
            return chosen_logps_detached, rejected_logps_detached, chosen_logps_avg_detached, rejected_logps_avg_detached
        else:
            return chosen_logps, rejected_logps, chosen_logps_avg, rejected_logps_avg, chosen_entroy, rejected_entropy, chosen_ppl, rejected_ppl

    def get_batch_metrics(self, batch, mode: str=None):
        """Compute the loss and other metrics for the given batch of inputs."""
        if self.config.use_reference:
            #we forward on reference model first, then clear out cache generated by reference model to save GPU memory
            with torch.no_grad():
                reference_chosen_logps, reference_rejected_logps, reference_chosen_logps_avg, reference_rejected_logps_avg = self.forward(self.reference_engine, batch, detach=True)

            policy_chosen_logps, policy_rejected_logps, policy_chosen_logps_avg, policy_rejected_logps_avg, policy_chosen_entropy, policy_rejected_entropy, policy_chosen_ppl, policy_rejected_ppl = self.forward(self.policy_engine, batch)
            if self.config.avg_logp:
                losses, chosen_logpderived_rewards, rejected_logpderived_rewards = self.loss(policy_chosen_logps_avg, policy_rejected_logps_avg, reference_chosen_logps_avg, reference_rejected_logps_avg)
            else:
                losses, chosen_logpderived_rewards, rejected_logpderived_rewards = self.loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps)
        
        else:
            policy_chosen_logps, policy_rejected_logps, policy_chosen_logps_avg, policy_rejected_logps_avg, policy_chosen_entropy, policy_rejected_entropy, policy_chosen_ppl, policy_rejected_ppl = self.forward(self.policy_engine, batch)
            if self.config.avg_logp:
                losses, chosen_logpderived_rewards, rejected_logpderived_rewards = self.loss(policy_chosen_logps_avg, policy_rejected_logps_avg)
            else:
                losses, chosen_logpderived_rewards, rejected_logpderived_rewards = self.loss(policy_chosen_logps, policy_rejected_logps)

        
        if self.config.online:
            datum_mask = torch.tensor(batch['valid'], dtype=torch.float32).to(self.local_rank)
            losses = losses * datum_mask

        
        KL_penalty = (policy_chosen_logps - reference_chosen_logps)/2 + (policy_rejected_logps-reference_rejected_logps)/2
           
        logpderived_reward_accuracies = (chosen_logpderived_rewards > rejected_logpderived_rewards).float()
        chosen_logpderived_rewards = all_gather_if_needed(chosen_logpderived_rewards, self.local_rank, self.world_size)
        rejected_logpderived_rewards = all_gather_if_needed(rejected_logpderived_rewards, self.local_rank, self.world_size)
        logpderived_reward_accuracies = all_gather_if_needed(logpderived_reward_accuracies, self.local_rank, self.world_size)
        policy_chosen_entropy = all_gather_if_needed(policy_chosen_entropy.detach(), self.local_rank, self.world_size)
        policy_rejected_entropy = all_gather_if_needed(policy_rejected_entropy.detach(), self.local_rank, self.world_size)
        all_devices_losses = all_gather_if_needed(losses.detach(), self.local_rank, self.world_size)
        chosen_response_len = all_gather_if_needed(torch.tensor(batch['chosen_input_ids'].shape[1], dtype=torch.float32).to(self.local_rank), self.local_rank, self.world_size, True)
        rejected_response_len = all_gather_if_needed(torch.tensor(batch['rejected_input_ids'].shape[1], dtype=torch.float32).to(self.local_rank), self.local_rank, self.world_size, True)
        policy_chosen_ppl = all_gather_if_needed(policy_chosen_ppl.detach(), self.local_rank, self.world_size)
        policy_rejected_ppl = all_gather_if_needed(policy_rejected_ppl.detach(), self.local_rank, self.world_size)
        KL_penalty = all_gather_if_needed(KL_penalty.detach(), self.local_rank, self.world_size)

        metrics = {}
        metrics[f'loss/{mode}'] = all_devices_losses.float().cpu().numpy().tolist()

        metrics[f'{mode}/logpderived_rewards_chosen'] = chosen_logpderived_rewards.float().cpu().numpy().tolist()
        metrics[f'{mode}/logpderived_rewards_rejected'] = rejected_logpderived_rewards.float().cpu().numpy().tolist()
        metrics[f'{mode}/logpderived_rewards_accuracies'] = logpderived_reward_accuracies.float().cpu().numpy().tolist()
        metrics[f'{mode}/logpderived_rewards_margins'] = (chosen_logpderived_rewards - rejected_logpderived_rewards).float().cpu().numpy().tolist()

        metrics[f'{mode}/chosen_entropy'] = policy_chosen_entropy.float().cpu().numpy().tolist()
        metrics[f'{mode}/rejected_entropy'] = policy_rejected_entropy.float().cpu().numpy().tolist()
        metrics[f'{mode}/chosen_length'] = chosen_response_len.float().cpu().numpy().tolist()
        metrics[f'{mode}/rejected_length'] = rejected_response_len.float().cpu().numpy().tolist()
        metrics[f'{mode}/chosen_ppl'] = policy_chosen_ppl.float().cpu().numpy().tolist()
        metrics[f'{mode}/rejected_ppl'] = policy_rejected_ppl.float().cpu().numpy().tolist()
        metrics[f'{mode}/KL'] = KL_penalty.float().cpu().numpy().tolist()

        
        del logpderived_reward_accuracies, chosen_logpderived_rewards, rejected_logpderived_rewards, all_devices_losses, policy_chosen_entropy, policy_rejected_entropy, KL_penalty
        
        #we average the loss over non-masked datums(i.e. two responses are not the same) for online training
        if self.config.online:
            num_counts = datum_mask.sum() if datum_mask.sum()>0 else 1
            
        loss = losses.sum()/num_counts if self.config.online else losses.mean()
        return loss, metrics


class DPOTrainer(PairedPreferenceTrainer):
    def loss(self,
            policy_chosen_logps: torch.FloatTensor,
            policy_rejected_logps: torch.FloatTensor,
            reference_chosen_logps: torch.FloatTensor,
            reference_rejected_logps: torch.FloatTensor):
        """Compute the DPO loss for a batch of policy and reference model log probabilities."""
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        
        # Note that the chosen rewards and rejected rewards are NOT given by reward model
        # They are calculated under the relationship between reward model and policy/reference model
        chosen_logpderived_rewards = self.config.beta * (policy_chosen_logps - reference_chosen_logps).detach().clone()
        rejected_logpderived_rewards = self.config.beta * (policy_rejected_logps - reference_rejected_logps).detach().clone()
        losses = -F.logsigmoid(self.config.beta * logits)

        return losses, chosen_logpderived_rewards, rejected_logpderived_rewards


class IPOTrainer(PairedPreferenceTrainer):    
    def loss(self,
            policy_chosen_logps: torch.FloatTensor,
            policy_rejected_logps: torch.FloatTensor,
            reference_chosen_logps: torch.FloatTensor,
            reference_rejected_logps: torch.FloatTensor):
        """Compute the DPO loss for a batch of policy and reference model log probabilities."""
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        # Note that the chosen rewards and rejected rewards are NOT given by reward model
        # They are calculated under the relationship between reward model and policy/reference model
        chosen_logpderived_rewards = self.config.tau * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_logpderived_rewards = self.config.tau * (policy_rejected_logps - reference_rejected_logps).detach()
        losses = (logits - 1/(2*self.config.tau))**2

        return losses, chosen_logpderived_rewards,  rejected_logpderived_rewards

class SLiCTrainer(PairedPreferenceTrainer):        
    def loss(self, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor):
        """Compute the SLIC loss as defined by Zhao et al. in https://arxiv.org/pdf/2305.10425.pdf

        Calibration loss defined as:
            L(x, y) := max(0, beta - log p_policy(y_chosen|x) + log p_policy(y_rejected|x))
        For the cross-entropy loss, just use the NLL of the chosen sequence (equivalent to SFT).
        """
        cal_loss = torch.clamp(self.config.beta - policy_chosen_logps + policy_rejected_logps, min=0)
        reg_loss = -policy_chosen_logps

        losses = cal_loss + self.config.lam * reg_loss

        chosen_logpderived_rewards = policy_chosen_logps.detach()
        rejected_logpderived_rewards = policy_rejected_logps.detach()

        return losses, chosen_logpderived_rewards, rejected_logpderived_rewards
    
class RewardTrainer(BasicTrainer):
    def loss(self, chosen_proxy_reward: torch.FloatTensor, rejected_proxy_reward: torch.FloatTensor):
        """
        Compute the reward modeling loss, which modeling the human preference via the Bradley-Terry assumption.
        """

        if self.config.reward_reg:
            return -F.logsigmoid(chosen_proxy_reward-rejected_proxy_reward) + self.config.reward_reg_val*(chosen_proxy_reward**2+rejected_proxy_reward**2)
        else:
            return -F.logsigmoid(chosen_proxy_reward-rejected_proxy_reward)
    

    def forward(self, model, batch):
        """Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.
           Return two tensors of shape (batch size), one of the chosen examples, another of the rejected ones.

           Returns:
            chosen_logps: log probabilities of chosen examples (should be batch size / 2 if data was read in correctly)
            rejected_logps: log probabilities of rejected examples (should be batch size / 2 if data was read in correctly)
        """
        concatenated_batch = self.concatenated_inputs(batch)
        rewards = model(concatenated_batch['concatenated_combined_input_ids'], attention_mask=concatenated_batch['concatenated_combined_attention_mask'])

        masks = (concatenated_batch['concatenated_labels'] != -100).detach().clone().to(self.reward_dtype).contiguous().to(self.local_rank)
        batch_size = masks.shape[0]
        last_token_idx = torch.zeros(batch_size).to(self.local_rank)

        for row in range(batch_size):
                last_token_idx[row] = masks[row].nonzero()[-1]
        
        last_token_reward = torch.gather(rewards, dim=1, index=last_token_idx.to(torch.int64).unsqueeze(1))
        
        chosen_proxy_rewards = last_token_reward[:batch['chosen_combined_input_ids'].shape[0]]
        rejected_proxy_rewards = last_token_reward[batch['chosen_combined_input_ids'].shape[0]:]

        return chosen_proxy_rewards.squeeze(-1).contiguous(), rejected_proxy_rewards.squeeze(-1).contiguous()

    def get_batch_metrics(self, batch, mode: str='train'):
        """Compute the loss and other metrics for the given batch of inputs."""
        metrics = {}
        chosen_proxy_reward, rejected_proxy_reward = self.forward(self.reward_engine, batch)
        losses = self.loss(chosen_proxy_reward=chosen_proxy_reward, rejected_proxy_reward=rejected_proxy_reward)

        reward_accuracies = (chosen_proxy_reward > rejected_proxy_reward).float()

        chosen_proxy_reward = all_gather_if_needed(chosen_proxy_reward.detach(), self.local_rank, self.world_size)
        rejected_proxy_reward = all_gather_if_needed(rejected_proxy_reward.detach(), self.local_rank, self.world_size)
        reward_accuracies = all_gather_if_needed(reward_accuracies.detach(), self.local_rank, self.world_size)
        all_devices_losses = all_gather_if_needed(losses.detach(), self.local_rank, self.world_size)

        metrics[f'{mode}/proxy_rewards_chosen'] = chosen_proxy_reward.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_rewards_rejected'] = rejected_proxy_reward.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_rewards_accuracies'] = reward_accuracies.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_rewards_margins'] = (chosen_proxy_reward - rejected_proxy_reward).float().cpu().numpy().tolist()
        metrics[f'loss/{mode}'] = all_devices_losses.float().cpu().numpy().tolist()

        del chosen_proxy_reward, rejected_proxy_reward, reward_accuracies, all_devices_losses
        return losses.mean(), metrics
    
    # def train(self):
    #     if not self.config.reward_statistics:
    #         super().train()
    #         return
        
    #     #After training, we calculate the mean and variance given by reward model on training set
    #     global_metrics = defaultdict(list)
    #     ### Relabel PART
    #     for batch in (tqdm.tqdm(self.train_iterator, desc='Calculating Reward mean and variance on training set') if self.global_rank==0 else self.train_iterator):
    #         self.set_eval_mode()
    #         batch = move_batch_on_device(batch, self.local_rank)
    #         with torch.no_grad():
    #             _, metrics = self.get_batch_metrics(batch, 'train')
            
    #         for k, v in metrics.items():
    #             global_metrics[k].extend(v)

    #         self.batch_counter += 1
    #         self.example_counter += self.config.global_batch_size
    #         del _
    #         remove_cache()
    #         mean, variance = calculate_mean_variance(global_metrics['train/proxy_rewards_chosen']+global_metrics['train/proxy_rewards_rejected'])

    #         self.log_message_rank0(f'Reward mean and standard variance on previous {self.example_counter} examples are:{mean:.3f}, {math.sqrt(variance):.3f}')

        
class RewardTrainerODIN(BasicTrainer):
    def Pearson_corr(self, x, y):
        covariance = torch.cov(torch.stack([x, y]))[0, 1]
        std_x = torch.std(x)
        std_y = torch.std(y)
        pearson_corr = covariance / (std_x * std_y)
        return pearson_corr
    
    def orthogonal(self, A, B):
        if A.shape[0]==0:
            return torch.zeros(1,).to(self.local_rank)
        # Compute A @ B.T
        M = torch.matmul(A, B.transpose(0,1))  

        A_norm = torch.norm(A, p="fro")
        B_norm = torch.norm(B, p="fro")
        M_norm = torch.norm(M, p="fro")

        # Orthogonal loss
        orthogonal_loss = M_norm / (A_norm*B_norm+1e-6)  # Square of Frobenius norm

        return orthogonal_loss
    
    def forward(self, model, batch):
        """Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.
           calculate all three losses

           Returns:
                chosen_proxy_reward_quality, rejected_proxy_reward_quality, reward_accuracies, all_devices_losses,  all_dev_length_loss,  all_dev_orthogonal_loss, all_dev_vanilla_loss
        """
        concatenated_batch = self.concatenated_inputs(batch)
        rewards_quality, rewards_length = model(concatenated_batch['concatenated_combined_input_ids'], attention_mask=concatenated_batch['concatenated_combined_attention_mask'])

        masks = (concatenated_batch['concatenated_labels'] != -100).detach().clone().to(self.reward_dtype).contiguous().to(self.local_rank)
        batch_size = masks.shape[0]
        last_token_idx = torch.zeros(batch_size).to(self.local_rank)

        for row in range(batch_size):
                last_token_idx[row] = masks[row].nonzero()[-1]
        
        ### The length loss
        #r_{\theta}^Q(x,y)
        last_token_reward_quality = torch.gather(rewards_quality, dim=1, index=last_token_idx.to(torch.int64).unsqueeze(1))
        #r_{\theta}^L(x,y)
        last_token_reward_length = torch.gather(rewards_length, dim=1, index=last_token_idx.to(torch.int64).unsqueeze(1))
        response_length = masks.sum(-1)

        alldev_last_token_reward_quality = all_gather_if_needed(last_token_reward_quality, self.local_rank, self.world_size).squeeze(-1).contiguous()
        alldev_last_token_reward_length = all_gather_if_needed(last_token_reward_length, self.local_rank, self.world_size).squeeze(-1).contiguous()
        alldev_response_length = all_gather_if_needed(response_length, self.local_rank, self.world_size).squeeze(-1).contiguous()
        length_loss = self.Pearson_corr(alldev_last_token_reward_quality, alldev_response_length).abs() - self.Pearson_corr(alldev_last_token_reward_length, alldev_response_length)

        ### The Orthogonal loss
        W_Q = model.module.scalar_head_quality.summary.weight
        W_L = model.module.scalar_head_length.summary.weight
        # self.log_message_rank0(model.module.scalar_head_quality)
        # self.log_message_rank0(model.module.scalar_head_length)

        # self.log_message_rank0(f"W_Q.shape is {W_Q.shape}")
        # self.log_message_rank0(f"W_L.shape is {W_L.shape}")
        orthogonal_loss = self.orthogonal(W_Q, W_L)

        ### The vanilla loss

        chosen_proxy_reward_quality = last_token_reward_quality[:batch['chosen_combined_input_ids'].shape[0]].squeeze(-1).contiguous()
        chosen_proxy_reward_length = last_token_reward_length[:batch['chosen_combined_input_ids'].shape[0]].squeeze(-1).contiguous()
        rejected_proxy_reward_quality = last_token_reward_quality[batch['chosen_combined_input_ids'].shape[0]:].squeeze(-1).contiguous()
        rejected_proxy_reward_length = last_token_reward_length[batch['chosen_combined_input_ids'].shape[0]:].squeeze(-1).contiguous()
        vanilla_loss = -F.logsigmoid(chosen_proxy_reward_quality+chosen_proxy_reward_length-rejected_proxy_reward_quality-rejected_proxy_reward_length)

        return length_loss, orthogonal_loss, vanilla_loss, chosen_proxy_reward_quality, rejected_proxy_reward_quality

    def get_batch_metrics(self, batch, mode: str='train'):
        """Compute the loss and other metrics for the given batch of inputs."""
        metrics = {}
        length_loss, orthogonal_loss, vanilla_loss, chosen_proxy_reward_quality, rejected_proxy_reward_quality = self.forward(self.reward_engine, batch)
        losses = vanilla_loss + self.config.reward_odin_L*length_loss + self.config.reward_odin_O*orthogonal_loss

        reward_accuracies = (chosen_proxy_reward_quality > rejected_proxy_reward_quality).float()
        chosen_proxy_reward_quality = all_gather_if_needed(chosen_proxy_reward_quality.detach(), self.local_rank, self.world_size)
        rejected_proxy_reward_quality = all_gather_if_needed(rejected_proxy_reward_quality.detach(), self.local_rank, self.world_size)
        reward_accuracies = all_gather_if_needed(reward_accuracies.detach(), self.local_rank, self.world_size)

        all_devices_losses = all_gather_if_needed(losses.detach(), self.local_rank, self.world_size)
        all_dev_length_loss = all_gather_if_needed(length_loss.detach(), self.local_rank, self.world_size)
        all_dev_orthogonal_loss =all_gather_if_needed(orthogonal_loss.detach(), self.local_rank, self.world_size)
        all_dev_vanilla_loss = all_gather_if_needed(vanilla_loss.detach(), self.local_rank, self.world_size)

        metrics[f'{mode}/proxy_reward_chosen_quality'] = chosen_proxy_reward_quality.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_reward_rejected_quality'] = rejected_proxy_reward_quality.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_reward_accuracies'] = reward_accuracies.float().cpu().numpy().tolist()
        metrics[f'{mode}/proxy_reward_margins_quality'] = (chosen_proxy_reward_quality - rejected_proxy_reward_quality).float().cpu().numpy().tolist()

        metrics[f'loss/{mode}'] = all_devices_losses.float().cpu().numpy().tolist()
        metrics[f'loss_length/{mode}'] = all_dev_length_loss.float().cpu().numpy().tolist()
        metrics[f'loss_orthogonal/{mode}'] = all_dev_orthogonal_loss.float().cpu().numpy().tolist()
        metrics[f'loss_vanilla/{mode}'] = all_dev_vanilla_loss.float().cpu().numpy().tolist()

        del chosen_proxy_reward_quality, rejected_proxy_reward_quality, reward_accuracies, all_devices_losses,  all_dev_length_loss,  all_dev_orthogonal_loss, all_dev_vanilla_loss
        return losses.mean(), metrics
    
class PreferenceTrainer(BasicTrainer):
    def __init__(self, config, tokenizer: AutoTokenizer, train_iterator: dataloader.DataLoader, eval_iterator: dataloader.DataLoader, policy_engine, reference_engine, reward_engine, critic_engine):
        super().__init__(config, tokenizer, train_iterator, eval_iterator, policy_engine, reference_engine, reward_engine, critic_engine)
        self.token_A_id = self.tokenizer.encode('A', return_tensors='pt')[0][1]
        self.token_B_id = self.tokenizer.encode('B', return_tensors='pt')[0][1]
    
    def get_batch_metrics(self, batch, mode: str='train'):
        """Compute the loss and other metrics for the given batch of inputs.
        Args:
            batch: dictionary of inputs for the batch (should contain 'target_attention_mask', 'target_input_input_ids', 
                'target_labels' where 'target' corresponds to the SFT example)
            mode: one of 'train', 'test'
        """
        metrics = {}  
        batch_size = len(batch['prompt_text'])     
        A_probs, B_probs = self.calculate_ntk_prob(self.policy_engine, batch) 
        A_probs, B_probs = A_probs.contiguous(), B_probs.contiguous()
        
        prefered = batch['target_text']
        accuracies = torch.zeros((batch_size,)).to(self.local_rank).contiguous()
        losses = torch.zeros((batch_size,)).to(self.local_rank).contiguous()

        for idx in range(batch_size):
            if (A_probs[idx]>B_probs[idx] and prefered[idx][0]=='A') or (A_probs[idx]<B_probs[idx] and prefered[idx][0]=='B'):
                accuracies[idx] = 1.0

            losses[idx] = -A_probs[idx].log() if prefered[idx][0]=='A' else -B_probs[idx].log()
            

        all_accuracies = all_gather_if_needed(accuracies, self.local_rank, self.world_size)
        all_Aprobs = all_gather_if_needed(A_probs.detach(), self.local_rank, self.world_size)
        all_Bprobs = all_gather_if_needed(B_probs.detach(), self.local_rank, self.world_size)
        all_devices_losses = all_gather_if_needed(losses.detach(), self.local_rank, self.world_size)

        metrics[f'{mode}/A_probs'] = all_Aprobs.float().cpu().numpy().tolist()
        metrics[f'{mode}/B_probs'] = all_Bprobs.float().cpu().numpy().tolist()
        metrics[f'{mode}/accuracy'] = all_accuracies.float().cpu().numpy().tolist()
        metrics[f'{mode}/loss'] = all_devices_losses.float().cpu().numpy().tolist()

        return losses.mean(), metrics 
    
    def calculate_ntk_prob(self, model, batch):
        """calculate next token probability of preference model on two responses.
        """

        logits = model(batch['prompt_input_ids'], attention_mask=batch['prompt_attention_mask']).logits
        last_token_logits = logits[:, -1, :]
        # Calculate the probability of the token
        softmax = torch.nn.Softmax(dim=1)
        probabilities = softmax(last_token_logits)

        # Get the probability of the specific token
        A_probs = probabilities[:,self.token_A_id]
        B_probs = probabilities[:,self.token_B_id]
        return A_probs, B_probs

    def extract_info(self, prompt):
        pattern = r"Title:\s*(.*?)\s*Post:\s*(.*?)\s*Summary-A:\s*(.*?)\s*Summary-B:\s*(.*?)\s*The better summary is:"
        # Use re.search to apply the pattern
        match = re.search(pattern, prompt, re.DOTALL|re.IGNORECASE)
        if match:
            title = match.group(1)
            post = match.group(2)
            summary_A = match.group(3)
            summary_B = match.group(4)
            
            return title, post, summary_A, summary_B
        else:
            return None

    def train(self):
        if not self.config.relabel:
            super().train()
            return
        
        #After training, we leverage the preference model to relabel the preference dataset
        all_examples = []
        failed_count = 0
        success_count = 0
        last_log_time = None
        ### Relabel PART
        for batch in (tqdm.tqdm(self.train_iterator, desc='Relabeling on training set\n') if self.global_rank==0 else self.train_iterator):
            start_time = time.time()
            batch_size = len(batch['prompt_text'])
            self.set_eval_mode()
            batch = move_batch_on_device(batch, self.local_rank)

            with torch.no_grad():
                _, metrics = self.get_batch_metrics(batch, 'test')
            batch_metrics = defaultdict(list)
            for k, v in metrics.items():
                batch_metrics[k].extend(v)

            mean_train_metrics = {}
            for k, v in batch_metrics.items():
                if len(v) > 0:
                    mean_train_metrics[k] = sum(v) / len(v)
         
            self.batch_counter += 1
            self.example_counter += self.config.global_batch_size
            del _
            remove_cache()

            with torch.no_grad():
                A_probs, B_probs = self.calculate_ntk_prob(self.policy_engine, batch)
            for idx in range(batch_size):
                try:
                    title, post, summary_a, summary_b = self.extract_info(batch['prompt_text'][idx])
                    choice = 0 if A_probs[idx]>B_probs[idx] else 1
                    all_examples.append({
                        'title': title,
                        'post': post,
                        'summaries': [summary_a, summary_b],
                        'choice': choice,
                        'worker': self.config.exp_name
                    })
                    success_count += 1
                except Exception as e:
                    failed_count += 1
                    self.log_message_rank0(f"{failed_count}/{failed_count+success_count}")

            step_time = time.time() - start_time
            examples_per_second = self.config.global_batch_size / step_time

            mean_train_metrics['counters/examples_per_second'] = examples_per_second
            mean_train_metrics['counters/examples'] = self.example_counter
            mean_train_metrics['counters/updates'] = self.batch_counter
            self.log_message_rank0(f'train stats after {self.example_counter} examples: {formatted_dict(mean_train_metrics)}')

            if self.config.wandb_enabled and self.global_rank==0:
                if last_log_time == None or time.time()-last_log_time>self.config.minimum_log_interval_secs:
                    wandb.log(mean_train_metrics, step=self.batch_counter)
                    last_log_time=time.time()
                else:
                    self.log_message_rank0('No wandb log to avoid log too frequently')

            delete_dict(batch_metrics)
            delete_dict(mean_train_metrics)

        os.makedirs(f'/data/data/tldr/relabeled_preference_{self.config.model_name}', exist_ok=True)
        with open(f'/data/data/tldr/relabeled_preference_{self.config.model_name}/{self.global_rank}.json', 'w') as f:
            json.dump(all_examples, f, indent=4)